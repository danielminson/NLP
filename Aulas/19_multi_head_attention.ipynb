{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-atenção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22516 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Activation, TimeDistributed, Softmax, MultiHeadAttention, TextVectorization, Reshape, RepeatVector, Conv1D, Bidirectional, AveragePooling1D, UpSampling1D, Embedding, Concatenate, GlobalAveragePooling1D, LSTM, Multiply\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# Ler um dataset e fazer batches\n",
    "DATASET_DIR = './datasets/frases/'\n",
    "\n",
    "from tensorflow.keras.utils import text_dataset_from_directory\n",
    "\n",
    "dataset = text_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    labels=None,\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    batch_size=2048,\n",
    "    max_length=None,\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    follow_links=False\n",
    ")\n",
    "\n",
    "from keras.layers import Input, TextVectorization\n",
    "from keras.models import Model\n",
    "vocab_size = 5000\n",
    "seq_len = 10\n",
    "vectorize_layer = TextVectorization(max_tokens=vocab_size, output_sequence_length=seq_len)\n",
    "vectorize_layer.adapt(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já vimos que um dos maiores problemas na modelagem linguística é manter algum tipo de coerência temporal nos tokens que são gerados.\n",
    "\n",
    "Um possível processo para modelar essa coerência temporal é o seguinte.\n",
    "\n",
    "Começaremos com três representações projetadas à partir da nossa entrada:\n",
    "\n",
    "$$\n",
    "Q = XW_q \\hspace{0.5in} V = XW_v \\hspace{0.5in} K = XW_k\n",
    "$$\n",
    "\n",
    "Depois, combinamos da seguinte forma:\n",
    "\n",
    "1. O produto interno $QK^T$ informa o quanto cada entrada, ao longo do tempo, depende das outras entradas,\n",
    "1. Essa dependência é escalada pela dimensão da representação de $X$ para evitar a explosão do espaço latente\n",
    "1. O resultado é ponderado por softmax, de forma que a soma das dependências ao longo do tempo é 1 e pode ser interpretada como uma probabilidade\n",
    "1. O resultado disso tudo pondera as representações $V$:\n",
    "\n",
    "$$\n",
    "S = D(Q, K, V) = \\text{softmax}\\begin{pmatrix} \\frac{QK^T}{\\sqrt{d_q}} \\end{pmatrix}V\n",
    "$$\n",
    "\n",
    "Esse processo pode ser feito em várias etapas paralelas que são somadas em uma mesma camada num processo chamado de *multi head*.\n",
    "\n",
    "## Exercício 1\n",
    "**Objetivo: analisar o processo de multi-head attention no Keras**\n",
    "\n",
    "Analisando o código abaixo, verifique:\n",
    "\n",
    "Quais são as entradas e saídas de um layer multi-head attention? O que cada dimensão significa?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)          [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 9, 15)        75000       ['input_17[0][0]']               \n",
      "                                                                                                  \n",
      " multi_head_attention_14 (Multi  (None, 9, 15)       393         ['embedding[0][0]',              \n",
      " HeadAttention)                                                   'embedding[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling1d_10 (G  (None, 15)          0           ['multi_head_attention_14[0][0]']\n",
      " lobalAveragePooling1D)                                                                           \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 5000)         80000       ['global_average_pooling1d_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " softmax_13 (Softmax)           (None, 5000)         0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 155,393\n",
      "Trainable params: 155,393\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_len = 10\n",
    "vocab_size = 5000\n",
    "def predict_word(seq_len, latent_dim, vocab_size):\n",
    "    input_layer = Input(shape=(seq_len-1,))\n",
    "    x = input_layer\n",
    "    x = Embedding(vocab_size, latent_dim, name='embedding', mask_zero=True)(x)\n",
    "    x = MultiHeadAttention(num_heads=3, key_dim=2)(x, value=x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    latent_rep = x\n",
    "    x = Dense(vocab_size)(x)\n",
    "    x = Softmax()(x)\n",
    "    return Model(input_layer, x), Model(input_layer, latent_rep)\n",
    "\n",
    "predictor, latent = predict_word(seq_len, 15, vocab_size)\n",
    "predictor.summary()\n",
    "#opt = keras.optimizers.SGD(learning_rate=1, momentum=0.9)\n",
    "opt = keras.optimizers.Nadam(learning_rate=0.1)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "    ignore_class=1,\n",
    "    name=\"sparse_categorical_crossentropy\",\n",
    ")\n",
    "\n",
    "predictor.compile(loss=loss_fn, optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 2\n",
    "**Objetivo: treinar e usar um modelo linguístico com multi-head attention**\n",
    "\n",
    "Usando o código abaixo, faça o treinamento de um modelo linguístico que usa multi-head attention. \n",
    "\n",
    "Após, use as funções que você já fez nas aulas anteriores para usar o modelo para gerar texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separar_ultimo_token(x):\n",
    "    x_ = vectorize_layer(x)\n",
    "    x_ = x_[:,:-1]\n",
    "    y_ = x_[:,-1:]\n",
    "    return x_, y_\n",
    "\n",
    "history = predictor.fit(dataset.map(separar_ultimo_token), epochs=40, verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3\n",
    "**Objetivo: criar um classificador de texto com multi-head attention**\n",
    "\n",
    "Usando a camada multi-head attention, projete e treine um classificador de texto para uma aplicação à sua escolha. Qual foi o accuracy que você obteve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
