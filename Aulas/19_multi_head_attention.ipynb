{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-atenção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19937 files belonging to 1 classes.\n",
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 11:16:23.357845: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Activation, TimeDistributed, Softmax, MultiHeadAttention, TextVectorization, Reshape, RepeatVector, Conv1D, Bidirectional, AveragePooling1D, UpSampling1D, Embedding, Concatenate, GlobalAveragePooling1D, LSTM, Multiply\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# Ler um dataset e fazer batches\n",
    "# DATASET_DIR = './datasets/frases/'\n",
    "DATASET_DIR = './datasets/frases_classificacao/train/positive/'\n",
    "\n",
    "from tensorflow.keras.utils import text_dataset_from_directory\n",
    "\n",
    "dataset = text_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    labels=None,\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    batch_size=2048,\n",
    "    max_length=None,\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    follow_links=False\n",
    ")\n",
    "\n",
    "from keras.layers import Input, TextVectorization\n",
    "from keras.models import Model\n",
    "vocab_size = 5000\n",
    "seq_len = 10\n",
    "vectorize_layer = TextVectorization(max_tokens=vocab_size, output_sequence_length=seq_len)\n",
    "vectorize_layer.adapt(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já vimos que um dos maiores problemas na modelagem linguística é manter algum tipo de coerência temporal nos tokens que são gerados.\n",
    "\n",
    "Um possível processo para modelar essa coerência temporal é o seguinte.\n",
    "\n",
    "Começaremos com três representações projetadas à partir da nossa entrada:\n",
    "\n",
    "$$\n",
    "Q = XW_q \\hspace{0.5in} V = XW_v \\hspace{0.5in} K = XW_k\n",
    "$$\n",
    "\n",
    "Depois, combinamos da seguinte forma:\n",
    "\n",
    "1. O produto interno $QK^T$ informa o quanto cada entrada, ao longo do tempo, depende das outras entradas,\n",
    "1. Essa dependência é escalada pela dimensão da representação de $X$ para evitar a explosão do espaço latente\n",
    "1. O resultado é ponderado por softmax, de forma que a soma das dependências ao longo do tempo é 1 e pode ser interpretada como uma probabilidade\n",
    "1. O resultado disso tudo pondera as representações $V$:\n",
    "\n",
    "$$\n",
    "S = D(Q, K, V) = \\text{softmax}\\begin{pmatrix} \\frac{QK^T}{\\sqrt{d_q}} \\end{pmatrix}V\n",
    "$$\n",
    "\n",
    "Esse processo pode ser feito em várias etapas paralelas que são somadas em uma mesma camada num processo chamado de *multi head*.\n",
    "\n",
    "## Exercício 1\n",
    "**Objetivo: analisar o processo de multi-head attention no Keras**\n",
    "\n",
    "Analisando o código abaixo, verifique:\n",
    "\n",
    "Quais são as entradas e saídas de um layer multi-head attention? O que cada dimensão significa?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 9, 15)        75000       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 9, 15)       393         ['embedding[0][0]',              \n",
      " dAttention)                                                      'embedding[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 15)          0           ['multi_head_attention[0][0]']   \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 5000)         80000       ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " softmax (Softmax)              (None, 5000)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 155,393\n",
      "Trainable params: 155,393\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_len = 10\n",
    "vocab_size = 5000\n",
    "def predict_word(seq_len, latent_dim, vocab_size):\n",
    "    input_layer = Input(shape=(seq_len-1,))\n",
    "    x = input_layer\n",
    "    x = Embedding(vocab_size, latent_dim, name='embedding', mask_zero=True)(x)\n",
    "    x = MultiHeadAttention(num_heads=3, key_dim=2)(x, value=x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    latent_rep = x\n",
    "    x = Dense(vocab_size)(x)\n",
    "    x = Softmax()(x)\n",
    "    return Model(input_layer, x), Model(input_layer, latent_rep)\n",
    "\n",
    "predictor, latent = predict_word(seq_len, 15, vocab_size)\n",
    "predictor.summary()\n",
    "#opt = keras.optimizers.SGD(learning_rate=1, momentum=0.9)\n",
    "opt = keras.optimizers.Nadam(learning_rate=0.1)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "    ignore_class=1,\n",
    "    name=\"sparse_categorical_crossentropy\",\n",
    ")\n",
    "\n",
    "predictor.compile(loss=loss_fn, optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 2\n",
    "**Objetivo: treinar e usar um modelo linguístico com multi-head attention**\n",
    "\n",
    "Usando o código abaixo, faça o treinamento de um modelo linguístico que usa multi-head attention. \n",
    "\n",
    "Após, use as funções que você já fez nas aulas anteriores para usar o modelo para gerar texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "10/10 [==============================] - 4s 167ms/step - loss: 7.3151 - accuracy: 0.0272\n",
      "Epoch 2/40\n",
      "10/10 [==============================] - 2s 131ms/step - loss: 6.0981 - accuracy: 0.0498\n",
      "Epoch 3/40\n",
      "10/10 [==============================] - 2s 117ms/step - loss: 6.0522 - accuracy: 0.0495\n",
      "Epoch 4/40\n",
      "10/10 [==============================] - 2s 117ms/step - loss: 5.8161 - accuracy: 0.0613\n",
      "Epoch 5/40\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 5.4498 - accuracy: 0.0798\n",
      "Epoch 6/40\n",
      "10/10 [==============================] - 2s 124ms/step - loss: 5.2334 - accuracy: 0.0956\n",
      "Epoch 7/40\n",
      "10/10 [==============================] - 2s 123ms/step - loss: 4.9742 - accuracy: 0.1080\n",
      "Epoch 8/40\n",
      "10/10 [==============================] - 2s 123ms/step - loss: 4.7425 - accuracy: 0.1253\n",
      "Epoch 9/40\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 4.7957 - accuracy: 0.1183\n",
      "Epoch 10/40\n",
      "10/10 [==============================] - 2s 124ms/step - loss: 4.4145 - accuracy: 0.1456\n",
      "Epoch 11/40\n",
      "10/10 [==============================] - 2s 121ms/step - loss: 4.3304 - accuracy: 0.1487\n",
      "Epoch 12/40\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 4.2026 - accuracy: 0.1592\n",
      "Epoch 13/40\n",
      "10/10 [==============================] - 2s 130ms/step - loss: 4.1934 - accuracy: 0.1608\n",
      "Epoch 14/40\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 4.0784 - accuracy: 0.1685\n",
      "Epoch 15/40\n",
      "10/10 [==============================] - 2s 123ms/step - loss: 4.0686 - accuracy: 0.1647\n",
      "Epoch 16/40\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 3.9219 - accuracy: 0.1866\n",
      "Epoch 17/40\n",
      "10/10 [==============================] - 2s 126ms/step - loss: 3.9369 - accuracy: 0.1792\n",
      "Epoch 18/40\n",
      "10/10 [==============================] - 2s 151ms/step - loss: 3.8380 - accuracy: 0.1873\n",
      "Epoch 19/40\n",
      "10/10 [==============================] - 2s 114ms/step - loss: 3.8047 - accuracy: 0.1966\n",
      "Epoch 20/40\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 3.6855 - accuracy: 0.2073\n",
      "Epoch 21/40\n",
      "10/10 [==============================] - 2s 114ms/step - loss: 3.6059 - accuracy: 0.2124\n",
      "Epoch 22/40\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 3.7035 - accuracy: 0.2036\n",
      "Epoch 23/40\n",
      "10/10 [==============================] - 2s 120ms/step - loss: 3.4852 - accuracy: 0.2239\n",
      "Epoch 24/40\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 3.4527 - accuracy: 0.2288\n",
      "Epoch 25/40\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 3.4235 - accuracy: 0.2348\n",
      "Epoch 26/40\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 3.5467 - accuracy: 0.2241\n",
      "Epoch 27/40\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 3.4109 - accuracy: 0.2384\n",
      "Epoch 28/40\n",
      "10/10 [==============================] - 2s 144ms/step - loss: 3.3980 - accuracy: 0.2414\n",
      "Epoch 29/40\n",
      "10/10 [==============================] - 2s 128ms/step - loss: 3.4086 - accuracy: 0.2333\n",
      "Epoch 30/40\n",
      "10/10 [==============================] - 2s 123ms/step - loss: 3.1719 - accuracy: 0.2652\n",
      "Epoch 31/40\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 3.2884 - accuracy: 0.2509\n",
      "Epoch 32/40\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 3.1508 - accuracy: 0.2656\n",
      "Epoch 33/40\n",
      "10/10 [==============================] - 2s 128ms/step - loss: 3.0508 - accuracy: 0.2805\n",
      "Epoch 34/40\n",
      "10/10 [==============================] - 2s 117ms/step - loss: 3.3007 - accuracy: 0.2479\n",
      "Epoch 35/40\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 3.2525 - accuracy: 0.2508\n",
      "Epoch 36/40\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 3.0784 - accuracy: 0.2754\n",
      "Epoch 37/40\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 2.9896 - accuracy: 0.2842\n",
      "Epoch 38/40\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 2.9979 - accuracy: 0.2836\n",
      "Epoch 39/40\n",
      "10/10 [==============================] - 2s 169ms/step - loss: 3.0474 - accuracy: 0.2772\n",
      "Epoch 40/40\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 2.9961 - accuracy: 0.2862\n"
     ]
    }
   ],
   "source": [
    "def separar_ultimo_token(x):\n",
    "    x_ = vectorize_layer(x)\n",
    "    x_ = x_[:,:-1]\n",
    "    y_ = x_[:,-1:]\n",
    "    return x_, y_\n",
    "\n",
    "history = predictor.fit(dataset.map(separar_ultimo_token), epochs=40, verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3\n",
    "**Objetivo: criar um classificador de texto com multi-head attention**\n",
    "\n",
    "Usando a camada multi-head attention, projete e treine um classificador de texto para uma aplicação à sua escolha. Qual foi o accuracy que você obteve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_and_downsample(input_n_samples, input_embedding_size, n_filters, kernel_size=3, **kwargs):\n",
    "    input_layer = Input(shape=(input_n_samples,input_embedding_size))\n",
    "    x = input_layer\n",
    "    x = Conv1D( filters=n_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                padding='same',\n",
    "                use_bias=False,\n",
    "                )(x)\n",
    "    x = AveragePooling1D(pool_size=2)(x)\n",
    "    x = Activation('elu')(x)\n",
    "    return Model(input_layer, x, **kwargs)\n",
    "\n",
    "seq_len = 10\n",
    "vocab_size = 5000\n",
    "def predict_word(seq_len, latent_dim, vocab_size):\n",
    "    input_layer = Input(shape=(seq_len-1,))\n",
    "    x = input_layer\n",
    "    x = Embedding(vocab_size, latent_dim, name='embedding', mask_zero=True)(x)\n",
    "    x = MultiHeadAttention(num_heads=3, key_dim=2)(x, value=x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = convolve_and_downsample(256, 2, number_of_ngrams, n_gram_size, name='ngramas')(x)\n",
    "    latent_rep = x\n",
    "    x = Dense(vocab_size)(x)\n",
    "    x = Softmax()(x)\n",
    "    return Model(input_layer, x), Model(input_layer, latent_rep)\n",
    "\n",
    "predictor, latent = predict_word(seq_len, 15, vocab_size)\n",
    "predictor.summary()\n",
    "#opt = keras.optimizers.SGD(learning_rate=1, momentum=0.9)\n",
    "opt = keras.optimizers.Nadam(learning_rate=0.1)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "    ignore_class=1,\n",
    "    name=\"sparse_categorical_crossentropy\",\n",
    ")\n",
    "\n",
    "predictor.compile(loss=loss_fn, optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
