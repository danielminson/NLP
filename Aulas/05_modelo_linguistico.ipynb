{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos linguísticos com n-gramas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até o momento, usamos modelos probabilísticos para a probabilidade de encontrar uma palavra $w$ em algum documento dentro da coleção $c$, isto é: $P(w | c)$. Implicitamente, isso quer dizer que a *ordem* das palavras dentro de um documento não tem impacto em seu significado. Metaforicamente, é como se colocássemos todas as palavras em uma grande sacola, e por isso esse tipo de representação baseado na presença ou não de palavras é chamado de *bag-of-words*.\n",
    "\n",
    "O modelo bag-of-words é eficaz para muitas aplicações, mas pode perder características importantes de uma palavra: por um lado, é pouco provável que um texto que mencione \"ornitorrincos\" e \"cangurus\" não se refira a ornitorrincos e cangurus; por outro lado, o texto: \"ornitorrincos são mais perigosos que cangurus\" é muito diferente de \"cangurus são mais perigosos que ornitorrincos\".\n",
    "\n",
    "Uma maneira de criar modelos para a *ordem* em que as palavras aparecem em um texto é chamado de modelo linguístico gerador (ou generativo ou gerativo, dependendo da tradução que você adotar) (*generative linguistic model*). Nesse tipo de modelo, estimamos a probabilidade de encontrar a $n$-ésima palavra de uma sequência com base na palavra anterior, isto é:\n",
    "\n",
    "$$\n",
    "P(w_n | w_{n-1})\n",
    "$$\n",
    "\n",
    "Podemos fazer um pequeno modelo para a passagem:\n",
    "\n",
    "    Passa um, passa dois, passa três\n",
    "\n",
    "Nesse caso, nosso modelo nos dá probabilidades como:\n",
    "\n",
    "* $P(\\text{passa} | \\text{um}) = 1$\n",
    "* $P(\\text{passa} | \\text{dois}) = 1$\n",
    "* $P(\\text{dois} | \\text{passa}) = 1/3$\n",
    "\n",
    "Veja que essas probabilidades são estimadas por contagem em um conjunto de dados de treino!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 1\n",
    "**Objetivo: calcular probabilidades de um modelo linguístico baseado em uma frase**\n",
    "\n",
    "Na passagem:\n",
    "\n",
    "    Joana foi passear com alguns de seus sete cachorros durante uma tarde ensolarada, e se encontrou com uma amiga. Uma pessoa ali presente também parou para conversar com elas, e também pararam alguns outros cachorros para brincar com os de Joana.\n",
    "\n",
    "Calcule:\n",
    "\n",
    "* $P(\\text{tarde} | \\text{uma}) =1/3$ a palavra \"uma\" apareceu 3 vezes, mas somente uma vez antes de \"tarde\"\n",
    "* $P(\\text{alguns} | \\text{com}) = 1/4$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto1 = \"Joana foi passear com alguns de seus sete cachorros durante uma tarde ensolarada, e se encontrou com uma amiga. Uma pessoa ali presente também parou para conversar com elas, e também pararam alguns outros cachorros para brincar com os de Joana.\"\n",
    "\n",
    "len(texto1.split(\" \"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 2\n",
    "**Objetivo: estimar um vocabulário à partir de dados**\n",
    "\n",
    "Usando reviews do `IMDB`, monte um vocabulário de todas as palavras que podem ser usadas para gerar reviews de filmes. Use tokens também para representar os sinais de pontuação, isto é, ` \"aqui, agora\"` deve ser tokenizado como `['aqui', ',', 'agora']`. Para isso, complete o código abaixo trocando a expressão regular que aparece no `re.findall`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "df = pd.read_csv('datasets/IMDB Dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101947\n"
     ]
    }
   ],
   "source": [
    "vocab = set()#é um conjunto\n",
    "# REGEX = r'(,\\s*[a-z]+)|([a-z]+\\s*,)'\n",
    "REGEX = r'\\b\\w+\\b|[,.!]'\n",
    "\n",
    "for text in df['review']:\n",
    "    tokens = re.findall(REGEX, text.lower())\n",
    "    vocab = vocab.union(set(tokens))\n",
    "print(len(vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3\n",
    "**Objetivo: estimar um modelo linguístico à partir de dados**\n",
    "\n",
    "O código abaixo realiza a contagem de tokens que seguem outros tokens usando a base de dados IMDB, bem como a quantidade total de aparições de cada token. Interprete o código e, depois, execute. Com base no resultado dele, estime:\n",
    "\n",
    "* A probabilidade de a palavra `under` ser seguida pela palavra `her`.\n",
    "* A probabilidade da palavra `violence` aparecer no fim de uma frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01886094674556213"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = {key: {'[TOTAL]':0} for key in vocab}\n",
    "\n",
    "for text in df['review']:\n",
    "    tokens = re.findall(REGEX, text.lower())\n",
    "    for i in range(len(tokens)-1):\n",
    "        wn = tokens[i+1]\n",
    "        wn1 = tokens[i]\n",
    "        if wn in model[wn1].keys():\n",
    "            model[wn1][wn] += 1\n",
    "        else:\n",
    "            model[wn1][wn] = 1\n",
    "            \n",
    "        model[wn1]['[TOTAL]'] += 1\n",
    "\n",
    "model[\"under\"][\"her\"]/model[\"under\"]['[TOTAL]']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 4\n",
    "**Objetivo: programar um sistema de sugestão de palavras**\n",
    "\n",
    "Com base no modelo que foi estimado no exercício anterior, programe uma função que recebe uma palavra e retorna uma possível palavra seguinte. Caso a palavra usada como base não faça parte do vocabulário do modelo, ele deve retornar uma palavra aleatória de seu vocabulário. Use a funcionalidade de `np.random.choice` para fazer escolhas com probabilidades pré-definidas, como abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "um\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.random.choice(['um', 'dois', 'tres'], p=[0.5, 0.2, 0.3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n"
     ]
    }
   ],
   "source": [
    "#t = 1 -> distribuicao original -> temperatura\n",
    "\n",
    "def gerar_palavras(model, palavra, t=0):\n",
    "    choices = [p for p in model[palavra].keys() if p!='[TOTAL]']\n",
    "    probs = [model[palavra][c]/model[palavra]['[TOTAL]'] for c in choices]\n",
    "    probs = np.array(probs)\n",
    "    probs = t * np.ones(probs.shape[0])/probs.shape[0]+(1-t) * probs\n",
    "    out = np.random.choice(choices, p = probs)\n",
    "    return out\n",
    "\n",
    "print(gerar_palavras(model, \"um\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 5\n",
    "**Objetivo: fazer um gerador de textos**\n",
    "\n",
    "Agora, podemos fazer um gerador de textos. Para isso, vamos gerar uma palavra, e então incorporá-la ao texto que temos, gerando palavras sucessivamente.\n",
    "\n",
    "Complete a função `gerar` abaixo para que ela retorne uma string com `n_tokens` novas palavras geradas à partir da string de entrada `s`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'genres , and got the 20th century . as good kids'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gerar(model, s, n_tokens):\n",
    "    string = ''\n",
    "    last_string = gerar_palavras(model, s)\n",
    "    string+=last_string\n",
    "    for i in range(n_tokens):\n",
    "        last_string = gerar_palavras(model, last_string)\n",
    "        string+=(\" \"+last_string)\n",
    "    return string\n",
    "\n",
    "gerar(model, \"western\", n_tokens=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 6\n",
    "**Objetivo: refletir sobre a aplicação de n-gramas em modelos linguísticos**\n",
    "\n",
    "Até o momento, usamos apenas uma palavra do passado para determinar o contexto que gera uma nova palavra. Poderíamos, porém, usar um número arbitrário $N$ de palavras anteriores, isto é:\n",
    "\n",
    "$$\n",
    "P(w_n | w_{n-1}, w_{n-2}, ... , w_{n-N})\n",
    "$$\n",
    "\n",
    "Podemos fazer um pequeno modelo para a passagem:\n",
    "\n",
    "    Passa um, passa dois, passa três\n",
    "\n",
    "Nesse caso, um modelo como o que temos feito até o moemnto nos daria probabilidades como:\n",
    "\n",
    "* $P(\\text{passa} | \\text{um}) = 1$\n",
    "* $P(\\text{passa} | \\text{dois}) = 1$\n",
    "* $P(\\text{dois} | \\text{passa}) = 1/3$\n",
    "\n",
    "Um modelo com $N=2$, por sua vez, teria:\n",
    "\n",
    "* $P(\\text{passa} | \\text{um, passa}) = 1$\n",
    "* $P(\\text{passa} | \\text{dois, passa}) = 1$\n",
    "* $P(\\text{dois} | \\text{passa, um}) = 1$\n",
    "\n",
    "Neste momento, aparece um novo elemento importante para nós. Nossa coleção não está mais sendo descrita por palavras, e sim por conjuntos de palavras ordenadas! Esses conjuntos são chamados de n-gramas.\n",
    "\n",
    "A função abaixo permite extrair n-gramas de uma lista de tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('isto', 'é', 'um'), ('é', 'um', 'teste'), ('um', 'teste', 'e'), ('teste', 'e', 'n-gramas'), ('e', 'n-gramas', 'podem'), ('n-gramas', 'podem', 'ser'), ('podem', 'ser', 'legais')]\n"
     ]
    }
   ],
   "source": [
    "def get_ngrams(tokens, N):#ngramas é um agrupamento de palavras\n",
    "    ngrams = [tuple(tokens[i:i+N]) for i in range(len(tokens)-N+1)]\n",
    "    return ngrams\n",
    "\n",
    "print(get_ngrams(\"isto é um teste e n-gramas podem ser legais\".split(), N=3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Qual é a vantagem em usar n-gramas com valores de $N$ maiores?\n",
    "1. Quando $N$ aumenta, qual é a probabilidade de encontrarmos ocorrências de n-gramas em mais de um documento?\n",
    "1. Como essa propriedade poderia ser utilizada para detectar cópias em trabalhos escritos? \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 7\n",
    "**Objetivo: usar n-gramas para melhorar nosso modelo linguístico**\n",
    "\n",
    "O código abaixo mostra um modelo linguístico para fazer predições de palavras com base nas $N$ palavras anteriores. O código, inicialmente, usa $N=2$.\n",
    "\n",
    "Implemente um gerador de textos baseado neste novo modelo linguístico.\n",
    "\n",
    "Caso as duas palavras que estejam no fim do texto recebido como entrada não estejam no banco de dados deste modelo, o sistema deve \"chamar\" o modelo anterior (baseado em somente uma palavra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams_and_res(tokens, N):\n",
    "    ngrams = [tuple(tokens[i:i+N]) for i in range(len(tokens)-N)]\n",
    "    res = [tokens[i+N] for i in range(len(tokens)-N)]\n",
    "    return ngrams, res\n",
    "\n",
    "# Criar vocabulário\n",
    "vocab = set()\n",
    "all_ngrams = []\n",
    "all_res = []\n",
    "for text in df['review'][0:1000]:\n",
    "    tokens = re.findall(REGEX, text.lower())\n",
    "    n_grams,res = get_ngrams_and_res(tokens, 2)\n",
    "    all_ngrams += n_grams\n",
    "    all_res += res\n",
    "    vocab = vocab.union(set(n_grams))\n",
    "\n",
    "# Criar modelo\n",
    "model_n2 = {key: {'[TOTAL]':0} for key in vocab}\n",
    "\n",
    "for i in range(len(all_ngrams)):\n",
    "    wn = all_res[i]\n",
    "    wn1 = all_ngrams[i]\n",
    "    if wn in model_n2[wn1].keys():\n",
    "        model_n2[wn1][wn] += 1\n",
    "    else:\n",
    "        model_n2[wn1][wn] = 1\n",
    "        \n",
    "    model_n2[wn1]['[TOTAL]'] += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 8\n",
    "**Objetivo: testar o gerador de textos em uma situação real**\n",
    "\n",
    "Escreva um trecho de um pequeno review de filmes em inglês. Use seu gerador para completar seu review. Como você avalia o desempenho dele?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
