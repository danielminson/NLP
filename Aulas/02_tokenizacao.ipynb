{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenização com RegEx\n",
    "\n",
    "Um problema que existe em processamento de linguagem natural é transformar uma string, que é algo que pode ser lido diretamente de textos, em uma lista de palavras individuais. Por exemplo, poderíamos querer transformar:\n",
    "\n",
    "`\"Minha string de entrada\"`\n",
    "\n",
    "em:\n",
    "\n",
    "`[\"Minha\", \"string\", \"de\", \"entrada\"]`\n",
    "\n",
    "Para isso, temos que toma algumas decisões, como:\n",
    "\n",
    "* As palavras são case-sensitive, isto é, diferenciamos caixa-alta de caixa-baixa?\n",
    "* O que fazemos com as pontuações?\n",
    "\n",
    "Nesta aula, vamos partir das expressões regulares e usar a biblioteca `re` para fazer tokenização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As onças pintadas são animais notáveis por seus hábitos de caça. Elas são caçadoras solitárias e usam sua habilidade de rastreamento para encontrar suas presas, geralmente animais como veados e javalis. O IBAMA, instituto responsável por proteger a fauna brasileira, monitora a população de onças e implementa medidas para protegê-las contra a caça ilegal. Além disso, o Ibama também trabalha para preservar o habitat dessas belas criaturas, garantindo que possam continuar a caçar de forma natural e sustentável. Ao preservarmos as onças pintadas e seus hábitos de caça, estamos protegendo a biodiversidade do nosso planeta.\n"
     ]
    }
   ],
   "source": [
    "entrada = \"As onças pintadas são animais notáveis por seus hábitos de caça. Elas são caçadoras solitárias e usam sua habilidade de rastreamento para encontrar suas presas, geralmente animais como veados e javalis. O IBAMA, instituto responsável por proteger a fauna brasileira, monitora a população de onças e implementa medidas para protegê-las contra a caça ilegal. Além disso, o Ibama também trabalha para preservar o habitat dessas belas criaturas, garantindo que possam continuar a caçar de forma natural e sustentável. Ao preservarmos as onças pintadas e seus hábitos de caça, estamos protegendo a biodiversidade do nosso planeta.\"\n",
    "print(entrada)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 1\n",
    "**Objetivo: usar a biblioteca string para separar palavras**\n",
    "\n",
    "Uma maneira de separar palavras em uma string é  usar a biblioteca `string`. Isso pode ser feito manualmente combinando:\n",
    "\n",
    "* `s = s.replace(x, y)`: troca todas as ocorrências da string `x` por `y` dentro da string `s`\n",
    "* `s = s.split()`: divide a string `s` em uma lista de strings usando os caracteres espaço e fim de linha.\n",
    "* `s = s.upper()`: converte a string `s` para caixa alta.\n",
    "\n",
    "Usando somente a biblioteca `string`, escreva um tokenizador para o texto da nossa entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AS', 'ONÇAS', 'PINTADAS', 'SÃO', 'ANIMAIS', 'NOTÁVEIS', 'POR', 'SEUS', 'HÁBITOS', 'DE', 'CAÇA', 'ELAS', 'SÃO', 'CAÇADORAS', 'SOLITÁRIAS', 'E', 'USAM', 'SUA', 'HABILIDADE', 'DE', 'RASTREAMENTO', 'PARA', 'ENCONTRAR', 'SUAS', 'PRESAS', 'GERALMENTE', 'ANIMAIS', 'COMO', 'VEADOS', 'E', 'JAVALIS', 'O', 'IBAMA', 'INSTITUTO', 'RESPONSÁVEL', 'POR', 'PROTEGER', 'A', 'FAUNA', 'BRASILEIRA', 'MONITORA', 'A', 'POPULAÇÃO', 'DE', 'ONÇAS', 'E', 'IMPLEMENTA', 'MEDIDAS', 'PARA', 'PROTEGÊ-LAS', 'CONTRA', 'A', 'CAÇA', 'ILEGAL', 'ALÉM', 'DISSO', 'O', 'IBAMA', 'TAMBÉM', 'TRABALHA', 'PARA', 'PRESERVAR', 'O', 'HABITAT', 'DESSAS', 'BELAS', 'CRIATURAS', 'GARANTINDO', 'QUE', 'POSSAM', 'CONTINUAR', 'A', 'CAÇAR', 'DE', 'FORMA', 'NATURAL', 'E', 'SUSTENTÁVEL', 'AO', 'PRESERVARMOS', 'AS', 'ONÇAS', 'PINTADAS', 'E', 'SEUS', 'HÁBITOS', 'DE', 'CAÇA', 'ESTAMOS', 'PROTEGENDO', 'A', 'BIODIVERSIDADE', 'DO', 'NOSSO', 'PLANETA']\n"
     ]
    }
   ],
   "source": [
    "s = entrada.replace('.', '').replace(',', '').upper().split()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 2\n",
    "**Objetivo: usar expressões regulares para fazer um tokenizador**\n",
    "\n",
    "A biblioteca `re` tem uma função chamada `findall` que retorna todas as ocorrências de uma determinada expressão regular dentro de uma string. Por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ola', 'Olaaaaaa', 'Olaaaaaaaaaaa']\n"
     ]
    }
   ],
   "source": [
    "s = re.findall(\"[Oo]la+\", 'Ola, mundo! Olaaaaaa! Olaaaaaaaaaaa??? Alguém aí?')\n",
    "print(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando `re.findall`, escreva um tokenizador semelhante ao que você escreveu anteriormente usando a biblioteca `string`. Os resultados devem ser iguais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AS', 'ONÇAS', 'PINTADAS', 'SÃO', 'ANIMAIS', 'NOTÁVEIS', 'POR', 'SEUS', 'HÁBITOS', 'DE', 'CAÇA', 'ELAS', 'SÃO', 'CAÇADORAS', 'SOLITÁRIAS', 'E', 'USAM', 'SUA', 'HABILIDADE', 'DE', 'RASTREAMENTO', 'PARA', 'ENCONTRAR', 'SUAS', 'PRESAS', 'GERALMENTE', 'ANIMAIS', 'COMO', 'VEADOS', 'E', 'JAVALIS', 'O', 'IBAMA', 'INSTITUTO', 'RESPONSÁVEL', 'POR', 'PROTEGER', 'A', 'FAUNA', 'BRASILEIRA', 'MONITORA', 'A', 'POPULAÇÃO', 'DE', 'ONÇAS', 'E', 'IMPLEMENTA', 'MEDIDAS', 'PARA', 'PROTEGÊ', 'LAS', 'CONTRA', 'A', 'CAÇA', 'ILEGAL', 'ALÉM', 'DISSO', 'O', 'IBAMA', 'TAMBÉM', 'TRABALHA', 'PARA', 'PRESERVAR', 'O', 'HABITAT', 'DESSAS', 'BELAS', 'CRIATURAS', 'GARANTINDO', 'QUE', 'POSSAM', 'CONTINUAR', 'A', 'CAÇAR', 'DE', 'FORMA', 'NATURAL', 'E', 'SUSTENTÁVEL', 'AO', 'PRESERVARMOS', 'AS', 'ONÇAS', 'PINTADAS', 'E', 'SEUS', 'HÁBITOS', 'DE', 'CAÇA', 'ESTAMOS', 'PROTEGENDO', 'A', 'BIODIVERSIDADE', 'DO', 'NOSSO', 'PLANETA']\n"
     ]
    }
   ],
   "source": [
    "# Faça o exercício aqui\n",
    "s = re.findall(r\"\\b\\w+\\b\", entrada.upper())#um ou mais\n",
    "print(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3\n",
    "**Objetivo: fazer um tokenizador assumindo que pontuações são tokens**\n",
    "\n",
    "Um problema da tokenização, na forma que foi feita, é que as pontuações são excluídas. Uma outra estratégia é considerar que pontuações são tokens, isto é:\n",
    "\n",
    "`\"uma frase, uma vírgula\"`\n",
    "\n",
    "seria tokenizado como:\n",
    "\n",
    "`[\"uma\", \"frase\", \",\", \"uma\", \"vírgula\"]`\n",
    "\n",
    "Escreva uma nova expressão para usar `findall` e tokenizar nossa frase de entrada. Se precisar, continue usando a função `upper()` da biblioteca `string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolva o exercício aqui"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 4\n",
    "**Objetivo: encontrar estatísticas de textos**\n",
    "\n",
    "O trecho de código abaixo faz o download do livro \"Quincas Borba\", de Machado de Assis, do [Portal Domínio Público](http://machado.mec.gov.br/obra-completa-lista), e então lê o PDF para uma string. Ele deve levar por volta de 1 minuto para executar, então execute a célula e siga para o restante do enunciado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\"http://machado.mec.gov.br/obra-completa-lista/item/download/14_7bbc6c42393beeac1fd963c16d935f40\", \"quincas.pdf\")\n",
    "texto = extract_text(\"quincas.pdf\")\n",
    "o = re.findall(r\"\\b\\w+\\b\", texto.upper())#um ou mais\n",
    "dict={}\n",
    "for palabra in o:\n",
    "    if palabra in dict.keys():\n",
    "        dict[palabra]+=1\n",
    "    else:\n",
    "        dict[palabra]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2836, 'QUE'), (2780, 'A'), (2425, 'O'), (2400, 'DE'), (2128, 'E'), (1499, 'NÃO'), (1215, 'SE'), (945, 'UM'), (899, 'DO'), (851, 'LHE')]\n"
     ]
    }
   ],
   "source": [
    "L = [(dict[k], k) for k in dict.keys()]\n",
    "\n",
    "L = []\n",
    "for k in dict.keys():\n",
    "    tupla = (dict[k], k)\n",
    "    L.append(tupla)\n",
    "\n",
    "Ls = sorted(L, reverse=True)\n",
    "print(Ls[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numero de palavras totais 10053\n"
     ]
    }
   ],
   "source": [
    "# len(texto)\n",
    "# len(set(texto))\n",
    "print(\"numero de palavras totais\",len(dict))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À partir das estratégias de tokenização que executamos, encontre:\n",
    "\n",
    "1. Quantas palavras há no texto?\n",
    "1. Quantas palavras *únicas* há no texto (isto é, cada palavra só é contada uma vez)?\n",
    "1. Quantas vezes cada palavra única aparece no texto?\n",
    "\n",
    "Dica: lembre-se das estruturas `set()` e `dict()` em Python!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 5\n",
    "**Objetivo: usar estatísticas de texto para comparar duas obras**\n",
    "\n",
    "O trecho abaixo faz o download e conversão do livro \"O pequeno Príncipe\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlretrieve(\u001b[39m\"\u001b[39m\u001b[39mhttps://www.sesirs.org.br/sites/default/files/paragraph--files/o_pequeno_principe_-_antoine_de_saint-exupery.pdf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprincipe.pdf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m texto2 \u001b[39m=\u001b[39m extract_text(\u001b[39m\"\u001b[39m\u001b[39mprincipe.pdf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m pequeno_principe \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mfindall(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m, texto\u001b[39m.\u001b[39mupper())\u001b[39m#um ou mais\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\"https://www.sesirs.org.br/sites/default/files/paragraph--files/o_pequeno_principe_-_antoine_de_saint-exupery.pdf\", \"principe.pdf\")\n",
    "texto2 = extract_text(\"principe.pdf\")\n",
    "pequeno_principe = re.findall(r\"\\b\\w+\\b\", texto.upper())#um ou mais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_prin={}\n",
    "for palabra in pequeno_principe:\n",
    "    if palabra in dict.keys():\n",
    "        dict_prin[palabra]+=1\n",
    "    else:\n",
    "        dict_prin[palabra]=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com base neste texto:\n",
    "\n",
    "1. Conte as palavras únicas de O Pequeno Príncipe usando o mesmo procedimento que você fez para \"Quincas Borba\".\n",
    "1. Encontre palavras que são muito frequentes em ambos os textos,\n",
    "1. Encontre palavras que são muito frequentes em um dos textos, mas que são pouco presentes no outro texto.\n",
    "1. A presença de algumas palavras pode ser usada para indicar o significado dos livros. Quais palavras seriam essas, em cada um dos livros?\n",
    "1. Algumas palavras são muito comuns, mas não tem um significado relacionado ao livro, como \"não\" ou \"muito\". Que palavras poderiam entrar nessa categoria?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
