{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from keras.layers import Input, Dense, Activation, TextVectorization, Embedding, GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vetorização e Embedding de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 1\n",
    "*Objetivo: entender como funciona a camada TextVectorization*\n",
    "\n",
    "Quando usamos sklearn, temos os objetos do tipo *vectorizer* que transformam strings em matrizes, em formatos que são \"compreensíveis\" por uma máquina. Em Keras, vamos usar a camada do tipo `TextVectorization` para o mesmo fim.\n",
    "\n",
    "No código abaixo:\n",
    "\n",
    "1. O que significa `output_mode='multi_hot'`?\n",
    "1. Como acessamos o vocabulário do vectorizer?\n",
    "1. O que significa o valor em `y[0,2]`?\n",
    "1. O que significa a palavra `[UNK]` no vocabulário do vectorizer?\n",
    "1. Da forma que está, o vetorizador é equivalente a qual construção do sklearn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dataset = [\"este é um texto\", \"outro texto este é\", \"outro texto só é peixe na virada da maré\"]\n",
    "\n",
    "text_vectorization = TextVectorization(output_mode='multi_hot')\n",
    "text_vectorization.adapt(toy_dataset)\n",
    "y = text_vectorization(toy_dataset)\n",
    "print(y)\n",
    "print(text_vectorization.get_vocabulary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 2\n",
    "*Objetivo: usar um vetorizador dentro do modelo de rede neural*\n",
    "\n",
    "O código abaixo incorpora o vetorizador ao modelo da rede neural de forma semelhante que o CountVectorizer e o LogisticRegression se acoplavam em uma pipeline.\n",
    "\n",
    "Em Keras, o vetorizador deve ser treinado em separado do restante da rede porque o treinamento do vetorizador é feito na CPU, ao passo que o restante da rede pode ser treinado na GPU.\n",
    "\n",
    "Outro detalhe importante é que a conversão dos rótulos para one-hot encoding deve ser feita fora do modelo (os modelos do Sklearn fazem essa operação internamente).\n",
    "\n",
    "No código abaixo:\n",
    "\n",
    "1. Aumente o número de épocas de treinamento para 30 e verifique o accuracy no conjunto de teste\n",
    "1. Modifique o `output_mode` do vetorizador para `count` e depois para `tf_idf`. Houve modificação no accuracy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/IMDB Dataset.csv')\n",
    "ohe = OneHotEncoder()\n",
    "y_ohe = ohe.fit_transform(df['sentiment'].to_numpy().reshape((-1,1))).todense()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], y_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "def multihot_softmax_model(vectorize_layer, vocab_size=vocab_size):\n",
    "    input_layer = Input(shape=(1,), dtype=tf.string)\n",
    "    x = input_layer\n",
    "    x = vectorize_layer(x)\n",
    "    x = Dense(2, name='classificador')(x)\n",
    "    x = Activation('softmax')(x)\n",
    "    return Model(input_layer, x)\n",
    "\n",
    "vectorize_layer = TextVectorization(output_mode='multi_hot', max_tokens=vocab_size, pad_to_max_tokens=True)\n",
    "vectorize_layer.adapt(X_train)\n",
    "clf = multihot_softmax_model(vectorize_layer)\n",
    "print(clf.summary())\n",
    "clf.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = clf.fit(X_train, y_train, epochs=30, verbose=1) # validation_split=0.1\n",
    "clf.evaluate(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 3\n",
    "*Objetivo: usar embeddings para representar documentos*\n",
    "\n",
    "Até o momento, estamos representando nossos documentos como sequências de *inteiros*, em que cada inteiro representa uma palavra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dataset = [\"este é um texto\", \"outro texto este é\", \"outro texto só é peixe na virada da maré\"]\n",
    "\n",
    "text_vectorization = TextVectorization(output_mode='int', max_tokens=vocab_size, pad_to_max_tokens=True, output_sequence_length=10)\n",
    "text_vectorization.adapt(toy_dataset)\n",
    "y = text_vectorization(toy_dataset)\n",
    "print(y)\n",
    "print(text_vectorization.get_vocabulary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimente colocar frases com palavras desconhecidas no trecho abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_vectorization([\"este texto tem um monte de palavras desconhecidas e isso realmente confunde nosso vetorizador\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vantagem da representação como sequência de inteiros é que preservamos a ordem das palavras. Uma desvantagem é que essa representação não é tão fácil de classificar como as que estávamos usando até agora.\n",
    "\n",
    "Uma solução para isso é mapear os inteiros para vetores - esses sim, num espaço vetorial fácil de classificar. Para isso, usamos uma camada chamada Embedding, que funciona exatamente como um dicionário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = Embedding(vocab_size, 2)\n",
    "y = text_vectorization([\"texto de exemplo\"])\n",
    "print(emb(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, temos mais um problema: a camada de embedding nos dá um tensor de dimensão: (batch x tempo x dimensão do embedding). Porém, nossa rede neural só consegue classificar elementos de dimensão (batch x features). Uma solução que podemos adotar para isso é tirar a média ao longo do tempo de todos os vetores do embedding, usando a camada GlobalAveragePooling1D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = GlobalAveragePooling1D()\n",
    "print(avg(emb(y)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas, você deve estar se perguntando: isso não vai fazer com que a informação de tempo seja jogada fora?\n",
    "\n",
    "Sim. Por enquanto, e só porque não sabemos lidar com o tempo *ainda*. Vamos usar a técnica atual para fazer um classificador e investigá-lo.\n",
    "\n",
    "---\n",
    "\n",
    "No código abaixo, encontre:\n",
    "\n",
    "1. Qual é a dimensão do embedding realizado.\n",
    "1. Quantos parâmetros existem na camada de classificação.\n",
    "1. Qual é o desempenho da rede após o treinamento?\n",
    "1. O que acontece se o parâmetro `output_sequence_length` for reduzido no vectorize_layer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_embedding_softmax_model(vectorize_layer, vocab_size=vocab_size):\n",
    "    input_layer = Input(shape=(1,), dtype=tf.string)\n",
    "    x = input_layer\n",
    "    x = vectorize_layer(x)\n",
    "    x = Embedding(vocab_size, 2, name='projecao')(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(2, name='classificador')(x)\n",
    "    x = Activation('softmax')(x)\n",
    "    return Model(input_layer, x)\n",
    "\n",
    "vectorize_layer = TextVectorization(output_mode='int', max_tokens=vocab_size, pad_to_max_tokens=True, output_sequence_length=200)\n",
    "vectorize_layer.adapt(X_train)\n",
    "clf = avg_embedding_softmax_model(vectorize_layer)\n",
    "print(clf.summary())\n",
    "clf.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = clf.fit(X_train, y_train, epochs=60, verbose=1) # validation_split=0.1\n",
    "clf.evaluate(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 4\n",
    "*Objetivo: visualizar e analisar as dimensões de embeddings atingidas pelo classificador*\n",
    "\n",
    "As projeções abaixo mostram a projeção (embedding) de cada palavra conforme foram atingidas pelo classificador.\n",
    "\n",
    "1. Analisando as figuras, você diria que aumentar o número de dimensões do embedding poderia levar a um ganho de desempenho no classificador?\n",
    "1. Analisando as figuras, você diria que é possível reduzir o número de dimensões do embedding para tornar nosso classificador ainda menor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Visualização: onde foi parar cada palavra?\n",
    "projecoes = clf.get_layer('projecao').get_weights()[0]\n",
    "vocabulario = vectorize_layer.get_vocabulary()\n",
    "y_pred_ohe = clf.predict(vocabulario)\n",
    "y_pred = ohe.inverse_transform(y_pred_ohe)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['dim_1'] = projecoes[:,0]\n",
    "df['dim_2'] = projecoes[:,1]\n",
    "df['word'] = vocabulario\n",
    "df['prediction'] = y_pred\n",
    "\n",
    "px.scatter(df, x=\"dim_1\", y=\"dim_2\", color=\"prediction\", hover_data=[\"word\"], title=\"Onde foi cada palavra?\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = pd.read_csv('embeddings_over_epochs.csv') # Gravei esse arquivo antecipadamente - demora muito para refazê-lo!\n",
    "df.head()\n",
    "df['prediction'][0] = 'positive' # Isso é definitivamente um hack. Sem isso, o plotly não vê o label 'positive' na primeira época, e começa a remover o label 'positive' dos plots subsequentes.\n",
    "\n",
    "px.scatter(df, x=\"dim_1\", y=\"dim_2\", animation_frame=\"epoch\", animation_group=\"word\",\n",
    "            color=\"prediction\", hover_name=\"word\", title=\"Training a 2D word embedding for sentiment analysis <br><sup>Where did each word go in the embedding space?</sup>\",\n",
    "          range_x=[-15,15], range_y=[-15,15],\n",
    "          width=800, height=800\n",
    "          )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 5\n",
    "*Objetivo: fazer um classificador de intenções*\n",
    "\n",
    "O código abaixo carrega um dataset de frases e intenções voltado para fazer um chatbot. Depois disso, ele seleciona as N_INTENTS intenções com mais exemplos na base de dados, simplificando o problema de classificação.\n",
    "\n",
    "Com base nesses dados e no que já vimos hoje:\n",
    "\n",
    "1. Programe, treine e avalie um classificador de intenções para o chatbot usando a topologia que vimos hoje. No treinamento, aumente o número de épocas até que o erro no conjunto de *validação* fique estável.\n",
    "1. Faça um plot do espaço de embeddings e verifique como as palavras se organizam nele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_URL = 'https://raw.githubusercontent.com/AmFamMLTeam/ACID/master/customer_training.csv'\n",
    "TEST_URL = 'https://raw.githubusercontent.com/AmFamMLTeam/ACID/master/customer_testing.csv'\n",
    "df_train = pd.read_csv(TRAIN_URL)\n",
    "df_test = pd.read_csv(TEST_URL)\n",
    "intent_count = df_train['INTENT_NAME'].value_counts()\n",
    "N_INTENTS = 3\n",
    "allowed_intents = intent_count[0:N_INTENTS].index\n",
    "filter_train = df_train['INTENT_NAME'].isin(allowed_intents)\n",
    "df_train_ = df_train[filter_train]\n",
    "filter_test = df_test['INTENT_NAME'].isin(allowed_intents)\n",
    "df_test_ = df_test[filter_test]\n",
    "print(allowed_intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "y_train = ohe.fit_transform(df_train_['INTENT_NAME'].to_numpy().reshape((-1,1))).todense()\n",
    "y_test = ohe.transform(df_test_['INTENT_NAME'].to_numpy().reshape((-1,1))).todense()\n",
    "X_train = df_train_['UTTERANCES']\n",
    "X_test = df_test_['UTTERANCES']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
